{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a98587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e428fc",
   "metadata": {},
   "source": [
    "## 1. Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8f6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f609269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "SAMPLING_RATE = 100 # 100Hz or 500Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae242dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotation data\n",
    "# See https://www.nature.com/articles/s41597-020-0495-6/tables/3 for columns present.\n",
    "database_file_path = os.path.join(DATA_DIR, \"ptbxl_database.csv\")\n",
    "Y = pd.read_csv(database_file_path, index_col=\"ecg_id\")\n",
    "\n",
    "# convert label dictionary\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8eb283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw signal data\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(os.path.join(path, f)) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(os.path.join(path, f)) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "X = load_raw_data(Y, SAMPLING_RATE, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18c2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scp_statements.csv for diagnostic aggregation\n",
    "# See https://www.nature.com/articles/s41597-020-0495-6/tables/13 for columns present.\n",
    "statements_file_path = os.path.join(DATA_DIR, \"scp_statements.csv\")\n",
    "agg_df = pd.read_csv(statements_file_path, index_col=0)\n",
    "agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "\n",
    "def aggregate_diagnostic(y_dic):\n",
    "    \"\"\"Aggregates and returns a list of superclasses present for each ECG.\"\"\"\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "    return list(set(tmp))\n",
    "\n",
    "# Apply diagnostic superclass\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "391988e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature data shape (X): (21799, 1000, 12)\n",
      "Label data shape (Y): (21799, 28)\n",
      "\n",
      "--- Class Distribution (Diagnostic Superclass) ---\n",
      "NORM    9514\n",
      "MI      5469\n",
      "STTC    5235\n",
      "CD      4898\n",
      "HYP     2649\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Samples by Number of Labels ---\n",
      "diagnostic_superclass\n",
      "0      411\n",
      "1    16244\n",
      "2     4068\n",
      "3      919\n",
      "4      157\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Missing Values in Metadata (Y) ---\n",
      "height                 14825\n",
      "weight                 12378\n",
      "nurse                   1473\n",
      "site                      17\n",
      "heart_axis              8468\n",
      "infarction_stadium1    16187\n",
      "infarction_stadium2    21696\n",
      "validated_by            9378\n",
      "baseline_drift         20201\n",
      "static_noise           18539\n",
      "burst_noise            21186\n",
      "electrodes_problems    21769\n",
      "extra_beats            19850\n",
      "pacemaker              21508\n",
      "dtype: int64\n",
      "\n",
      "--- Missing/Corrupt Values in Signal Data (X) ---\n",
      "X contains NaNs: False\n",
      "X contains Infinite values: False\n"
     ]
    }
   ],
   "source": [
    "## Info about sets\n",
    "print(f\"Feature data shape (X): {X.shape}\") # (Samples, Timepoints, Leads)\n",
    "print(f\"Label data shape (Y): {Y.shape}\")\n",
    "\n",
    "# Flatten the list of diagnostic superclasses to get a total count\n",
    "all_diagnostics = [item for sublist in Y.diagnostic_superclass for item in sublist]\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = pd.Series(all_diagnostics).value_counts()\n",
    "print(\"\\n--- Class Distribution (Diagnostic Superclass) ---\")\n",
    "print(class_counts)\n",
    "\n",
    "# Check how many samples have multiple superclasses\n",
    "multi_label_counts = Y.diagnostic_superclass.apply(len).value_counts().sort_index()\n",
    "print(\"\\n--- Samples by Number of Labels ---\")\n",
    "print(multi_label_counts)\n",
    "\n",
    "print(\"\\n--- Missing Values in Metadata (Y) ---\")\n",
    "# Check for nulls in the metadata\n",
    "missing_meta = Y.isnull().sum()\n",
    "print(missing_meta[missing_meta > 0]) # Only print columns with missing data\n",
    "\n",
    "print(\"\\n--- Missing/Corrupt Values in Signal Data (X) ---\")\n",
    "# Check if there are any NaNs or Infinite values in the numpy array\n",
    "has_nans = np.isnan(X).any()\n",
    "has_infs = np.isinf(X).any()\n",
    "print(f\"X contains NaNs: {has_nans}\")\n",
    "print(f\"X contains Infinite values: {has_infs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33bbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "test_fold = 10\n",
    "# Train\n",
    "X_train = X[np.where(Y.strat_fold != test_fold)]\n",
    "y_train = Y[(Y.strat_fold != test_fold)].diagnostic_superclass\n",
    "# Test\n",
    "X_test = X[np.where(Y.strat_fold == test_fold)]\n",
    "y_test = Y[Y.strat_fold == test_fold].diagnostic_superclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae815aee",
   "metadata": {},
   "source": [
    "## 2. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b46faa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739ebc16",
   "metadata": {},
   "source": [
    "### 2.1 Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b634590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes found: ['CD' 'HYP' 'MI' 'NORM' 'STTC']\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_bin = mlb.fit_transform(y_train)\n",
    "y_test_bin = mlb.transform(y_test)\n",
    "num_classes = len(mlb.classes_)\n",
    "print(f\"\\nClasses found: {mlb.classes_}\")\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "# X input is (Samples, Time, Leads)\n",
    "# PyTorch Conv1d expects (Samples, Leads, Time).\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32).permute(0, 2, 1)\n",
    "y_train_t = torch.tensor(y_train_bin, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test_bin, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4dde3d",
   "metadata": {},
   "source": [
    "### 2.2 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b482f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n"
     ]
    }
   ],
   "source": [
    "class ECGNet(nn.Module):\n",
    "    def __init__(self, num_leads, num_classes):\n",
    "        super(ECGNet, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_leads, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Global Average Pooling (handles variable length if needed, averages over time axis)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, 12, 1000]\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = self.gap(x) # Shape: [Batch, 128, 1]\n",
    "        x = x.view(x.size(0), -1) # Flatten: [Batch, 128]\n",
    "        x = self.fc(x) # Output logits\n",
    "        return x\n",
    "\n",
    "# Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "model = ECGNet(num_leads=12, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec66b3",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b9009fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/10 - Loss: 0.3656\n",
      "Epoch 2/10 - Loss: 0.3132\n",
      "Epoch 3/10 - Loss: 0.2972\n",
      "Epoch 4/10 - Loss: 0.2885\n",
      "Epoch 5/10 - Loss: 0.2823\n",
      "Epoch 6/10 - Loss: 0.2755\n",
      "Epoch 7/10 - Loss: 0.2726\n",
      "Epoch 8/10 - Loss: 0.2687\n",
      "Epoch 9/10 - Loss: 0.2658\n",
      "Epoch 10/10 - Loss: 0.2617\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "EPOCHS = 10\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af49af",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a19e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating on Test Set ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # get probabilities\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        \n",
    "        all_preds.append(probs.cpu().numpy())\n",
    "        all_targets.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate batches\n",
    "y_pred_probs = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_targets)\n",
    "\n",
    "# Convert probabilities to binary predictions (Threshold = 0.5)\n",
    "y_pred_bin = (y_pred_probs > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b402dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.81      0.63      0.71       496\n",
      "         HYP       0.79      0.39      0.52       262\n",
      "          MI       0.76      0.70      0.73       550\n",
      "        NORM       0.81      0.91      0.85       963\n",
      "        STTC       0.79      0.62      0.69       521\n",
      "\n",
      "   micro avg       0.80      0.71      0.75      2792\n",
      "   macro avg       0.79      0.65      0.70      2792\n",
      "weighted avg       0.80      0.71      0.74      2792\n",
      " samples avg       0.74      0.72      0.72      2792\n",
      "\n",
      "Macro ROC AUC Score: 0.9152\n"
     ]
    }
   ],
   "source": [
    "# Classification Report (Precision, Recall, F1 per class)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_bin, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# Macro AUROC\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs, average=\"macro\")\n",
    "    print(f\"Macro ROC AUC Score: {roc_auc:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"Could not calculate ROC AUC (possibly only one class present in test subset).\")\n",
    "\n",
    "# Save model\n",
    "model_dir = \"../models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, \"ecg_model.pth\")\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-ecg-optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
